---
title: "2022-2 통계적학습및실습 중간시험"
author: "기지훈"
date: '201610303'
output:
  html_document:
    df_print: paged
---


우리는 `ISLR2` 패키지 안에 있는 `Boston` 데이터를 이용할거에요. 여기서는 범죄율이 1\%보다 높은 경우 `TRUE`값을 갖고, 그렇지 않은 경우 `FALSE`값을 갖는 새로운 변수 `c_rate`를 생성하고, 범죄율이 높은 지역과 낮은 지역을 예측하는 최적의 모델을 구축해보겠습니다. 먼저 아래의 코드를 실행하세요.

```{r}
library(ISLR2)
library(MASS)
library(class)
library(glmnet)
boston<-Boston
boston$c_rate<-(boston$crim>1)
boston<-boston[,-c(1,2)]
```

`boston`데이터의 구조를 살펴보기 위해 처음 6행을 출력해볼게요. 그리고 `boston` 데이터의 차원과 `boston`데이터에 있는 변수들의 이름을 살펴봅시다.

```{r}
head(boston,6)
dim(boston)
names(boston)
```

편의를 위해 `boston` 데이터를 `attach()` 함수를 이용하여 R에 붙여넣읍시다. 
```{r}
attach(boston)
```

`boston`을 훈련자료 `boston.train`과 시험자료 `boston.test`로 나눠봅시다. seed 번호는 1로 하고, 훈련자료의 갯수는 400개, 시험자료의 갯수는 나머지로 하겠습니다. 반응변수 `c_rate`를 제외한 나머지 설명변수들을 표준화해줍시다.

```{r}
set.seed(1)
train_ind<-sample(dim(boston)[1], 400, replace = FALSE)
boston.train<-boston[train_ind,]
boston.test<-boston[-train_ind,]
boston.train[,-dim(boston)[2]]<-scale(boston.train[,-dim(boston)[2]])
boston.test[,-dim(boston)[2]]<-scale(boston.test[,-dim(boston)[2]])
```

`nox`, `rm`, `age`, `dis`, `rad`, `black`, `medv` 변수를 설명변수로 하는 로지스틱 회귀모형을 `logit.model`이라는 이름으로 만들어보세요(후진 단계적 선택법에 의해 선택된 변수입니다). 아래의 `form`을 공식에 사용하면 편리할거에요. 

```{r}
form <- as.formula('c_rate ~ nox + rm + age + dis + rad + black + medv')

```


```{r}

(logit.model<-glm(form,data=boston.train,family = "binomial"))

```

`logit.model`을 이용하여 훈련자료에서 `TRUE`일 확률을 구하고 `logit.probs`에 저장하세요. 예측확률이 0.5보다 크면 `TRUE` 인가요? `FALSE` 인가요? 어떻게 알 수 있나요?

```{r}

logit.probs <- predict(logit.model,newdata=boston.train, type = "response")
(contrasts(c_rate) )  ## TRUE가 1 FAlSE가 0으로 모델을 만들었으므로  0.5보다 크면 TRUE이다

```
훈련자료에서의 예측결과에 대한 혼동행렬을 구하고, 오분류율을 계산해보세요. 
```{r}

glm.pred <- rep(FALSE, 400)
glm.pred[logit.probs > .5] = TRUE
glm.pred<-(logit.probs>0.5)
(table(glm.pred,boston.train$c_rate))
##오분류율
(4+12)/400

```
이제 시험자료에 적용하여 예측해봅시다. 혼동행렬을 구하고 오분류율을 계산해보세요. 

```{r}

glm.pred1<-rep(FALSE,nrow(boston.test))
logit.probs.1<-predict(logit.model,boston.test,type="response")
glm.pred1[logit.probs.1 > .5] = TRUE
(table(glm.pred1,boston.test$c_rate))
##오분류율
(4+2)/106

```

동일한 설명변수들을 이용하여 선형판별분석을 수행하고 결과를 `lda.fit`에 저장하세요. 앞에서와 마찬가지로 모형을 만들 때는 훈련자료들만을 이용합니다. 훈련자료에서의 혼동행렬과 오분류율을 구하세요. 

```{r}

lda.fit <- lda(form, data = boston.train)
lda.probs<-predict(lda.fit,type="response")
lda.class<-lda.probs$class
(table(lda.class,boston.train$c_rate))
##오분류율
(21/400)

```
시험자료에서 선형판별분석모델의 혼동행렬과 오분류율을 구하세요. 

```{r}

lda.probs1<-predict(lda.fit,boston.test)
lda.class1<-lda.probs1$class
table(lda.class1,boston.test$c_rate)
##오분류율
5/106

```
이차판별분석모델도 위와 동일하게 수행해보세요. 결과를 `qda.fit`에 저장하세요. 훈련자료에서의 혼동행렬과 오분류율을 구하세요. 

```{r}

qda.fit <- qda(form,data=boston.train)
qda.pred<-predict(qda.fit,type="response")
qda.class<-qda.pred$class
table(qda.class,boston.train$c_rate)
##오분류율
(3+15)/400

```
마찬가지로 시험자료에서 이차판별분석모델의 혼동행렬과 오분류율을 구하세요. 

```{r}

qda.pred.1<-predict(qda.fit,boston.test)
qda.class.1<-qda.pred.1$class
table(qda.class.1,boston.test$c_rate)
##오분류율
(2+2)/106

```


이제 K-최근접 이웃방법을 적용해보려고 합니다. `class` 패키지의 `knn()`함수를 이용하기 위해서는 훈련자료와 시험자료를 새롭게 생성해야 합니다. `model.matrix()` 함수를 이용하면 편리할꺼에요. (절편을 포함하지 않도록 주의합니다.)

```{r}
x.train <- model.matrix(form, boston.train)[,-1]
y.train <- boston.train$c_rate
x.test  <- model.matrix(form, boston.test)[,-1]
y.test <- boston.test$c_rate
```

다음은 최적의 이웃의 수 $K$값을 결정하기 위해 LOOCV(leave-one-out cross validation)을 수행하는 코드입니다.
`loocv_knn`에는 각 $K$값 (`k_candidate`)에서의 LOOCV 오분류율이 기록되어 있습니다. LOOCV 오분류율이 가장 작은 $K$값은 얼마입니까?
```{r}
k_candidate<-seq(3,19,2)
loocv_knn<-c()
for(i in 1:length(k_candidate)){
  loocv_knn[i]<-mean(y.train!=knn.cv(x.train, y.train, k = k_candidate[i]))
}
loocv_knn

k=3
```

위에서의 $K$값을 이용하여 시험자료에서의 K-최근접 이웃 분류결과를 `knn.test`에 저장하세요. 시험자료에서의 KNN 분류결과의 혼동행렬과 오분류율을 구하세요.

```{r}
knn.test <- knn(x.train,x.test, y.train, k = k)
table(knn.test, y.test)
##오분류율
(2+2)/106
```

전체 변수들을 활용하여 로지스틱 능형회귀와 라쏘를 적용해봅시다. 먼저 전체 변수들을 이용하여 `x.train2`와 `x.test2`를 만드세요. 
```{r}
x.train2 <- model.matrix(c_rate~., boston.train)[,-1]
y.train2 <-boston.train$c_rate
x.test2  <- model.matrix(c_rate~., boston.test)[,-1]
y.test2 <- boston.test$c_rate
```



후보 $\lambda$ 값으로 $10$부터 $-10$까지 동일한 간격으로 이루어진 수열을 `exp(1)`의 거듭제곱으로 한 값을 고려하겠습니다 (즉, $e^{a_1},e^{a_2}, \dots, e^{a_{100}}$).`ridge`에 능형회귀 결과를 저장하세요. 
(`glmnet` 함수를 이용하며, `family = binomial`을 추가하면 됩니다. )

```{r}
grid = exp(1)^(seq(10,-10, length = 100))
ridge<-glmnet(x.train2,y.train2,alpha=0,lambda=grid,family = 'binomial')

```

$\log \lambda$를 $x$축으로한 정규화 그림을 그려봅시다. 단순히 옵션을 `xvar='lambda' `를 주면 될거에요.
```{r}
plot(ridge,xvar='lambda')

```
교차검증을 통해 최적의 $\lambda$를 선택해봅시다 seed번호를 5로 하고 8단 교차검증을 수행해서 `cv.ridge`에 저장하세요. 교차검증오차가 최소가 되게 하는 $\lambda$를 `bestlam`에 저장하세요.

```{r}
set.seed(5)
  cv.ridge <- cv.glmnet(x.train2,y.train2,nfold=8,alpha=0)
plot(cv.ridge)
bestlam <- cv.ridge$lambda.min
bestlam
```

`lambda = bestlam`을 사용하여 시험자료를 분류하고 `ridge.pred`에 저장해봅시다. 일반적인 로지스틱 회귀에서 했던것 처럼 `type = "response"` 를 지정하여 먼저 저장하고, 다시 확률을 이용해서 분류해야 한다는 것을 기억하기 바랍니다. 

혼동행렬과 오분류율을 구하세요.

```{r}
ridge.probs<-predict(ridge,x.test2,type = "response", s = bestlam)
ridge.class<-rep(FALSE,106)
ridge.class[ridge.probs > .5] = TRUE
(table(ridge.class,y.test2)) 
#오분류율
(1+3)/106
```
이제 라쏘에 대해서도 위의 절차를 반복해봅시다. 먼저 `lasso`에 적합결과를 저장합시다. 능형회귀에서와 동일한 `lambda`를 사용합니다. 

```{r}
lasso<-glmnet(x.train2,y.train2,alpha=1,lambda=grid,family = 'binomial')
```


$\log \lambda$를 $x$축으로한 정규화 그림을 그려봅시다. 단순히 옵션을 `xvar='lambda' `를 주면 될거에요.

```{r}
plot(lasso,xvar='lambda')
```

교차검증을 통해 최적의 $\lambda$를 선택해봅시다 seed번호를 5번으로하고 8단 교차검증을 수행해서 `cv.lasso`에 저장하세요. 교차검증오차가 최소가 되게 하는 $\lambda$를 `bestlam.lasso`에 저장하세요.


```{r}
set.seed(5)
  cv.lasso<- cv.glmnet(x.train2,y.train2,nfold=8,alpha=1)
plot(cv.lasso)
bestlam.lasso<-cv.lasso$lambda.min
bestlam.lasso
```


$\lambda$=`bestlam.lasso`에서의 변수선택 결과는 어떻게 되나요? `predict` 함수와 `type="coefficients"` 인수를 활용하면 살펴볼 수 있을거에요. 

```{r}
(predict(lasso,s=bestlam.lasso,type="coefficients"))
## tax, lstat 변수가 제거된다.
```

`lambda = bestlam.lasso`을 사용하여 시험자료를 분류하고 `lasso.pred`에 저장해봅시다. 앞선 능형회귀에서와 유사하게 진행하면 됩니다. 혼동행렬과 오분류율을 구하세요.

```{r}
lasso.pred<-predict(lasso,x.test2,type = "response", s = bestlam.lasso)
lasso.class<-rep(FALSE,106)
lasso.class[lasso.pred > .5] = TRUE
(table(lasso.class,y.test2)) 

##오분류율 
(2+1)/106
```