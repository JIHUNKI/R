---
title: "통계학특강"
author: "강종경"
header-includes: \usepackage{setspace}\doublespacing
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: ["kotex","amsmath"]
    keep_tex: yes
  word_document: default
  html_document:
    df_print: paged
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.2
  kernelspec:
    display_name: R
    language: R
    name: ir
mainfont: NanumGothic
editor_options:
  markdown:
    wrap: sentence
fontsize: 12pt
---


\newcommand{\ba}{\boldmath{a}}
\newcommand{\bA}{\boldmath{A}}
\newcommand{\bx}{\boldmath{x}}
\newcommand{\bX}{\boldmath{X}}
\newcommand{\by}{\boldmath{y}}
\newcommand{\bY}{\boldmath{Y}}
\newcommand{\bH}{\boldmath{H}}
\newcommand{\bI}{\boldmath{I}}
\newcommand{\bP}{\boldmath{P}}
\newcommand{\bv}{\boldmath{v}}
\newcommand{\bw}{\boldmath{w}}
\newcommand{\bz}{\boldmath{z}}
\newcommand{\bS}{\boldmath{S}}
\newcommand{\bR}{\boldmath{R}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}

# 모형 선택과 벌점화2(Model Selection and Regularization)

## 차원축소 방법(Dimension Reduction Method)


차원 축소 방법은 예측 변수를 변환한 다음 원래 예측 변수 대신 변환된 변수를 사용하여 최소 제곱 모형을 적합시키는 기법입니다.
$Z_1, Z_2, \dots, Z_m$을 $m<p$인 원변수들을 선형결합한 변수들이라고 합시다. 예를들어 $Z_m$은 다음과 같이 표현할 수 있습니다.

$$
Z_m =\sum_{j=1}^{p}\phi_{jm}X_j
$$

여기서 $\phi_{jm}$은 적절한 상수입니다. 이렇게 재표현한 변수들을 이용하여 다음과 같은 선형모형을 생각해봅시다.

$$
y_i = \theta_0 +\sum_{k=1}^{m}\theta_kZ_{ik}+\epsilon_i
$$
여기서 $i=1, \dots, n$이며, $\theta_0, \theta_1, \dots, \theta_m$은 회귀계수입니다. 만일 $\theta_{1}, \dots, \theta_{m}$이 적절하게 선택된다면 차원 축소 방법은 원래의 변수들만을 활용한 최소제곱회귀보다 더 나은 성능을 보일 수 있습니다. 차원 축소기법을 활용하면 원래의 $p+1$개의 설명변수를 이용하는 대신 $m+1$개의 설명변수들만을 활용할 수 있기 때문입니다.

다음 식을 살펴봅시다.

$$
\sum_{k=1}^{m} \theta_{k} Z_{i k}=\sum_{k=1}^{m} \theta_{k} \sum_{j=1}^{p}\phi_{j k} X_{i j}=\sum_{j=1}^{p} \sum_{k=1}^{m} \theta_{k} \phi_{j k} X_{i j}=\sum_{j=1}^{p} \beta_{j} X_{i j}
$$
여기서
$$
\beta_j =\sum_{k=1}^{m}\theta_k\phi_{jk}
$$
입니다.

이 제약 조건은 계수 추정치를 편향시킬 가능성이 있지만, $p$가 $n$에 비해 큰 상황에서는 $p$보다 훨씬 작은 $m$의 값을 선택하면 적합 계수의 분산을 유의하게 줄일 수 있습니다.

만약 $m=p$이고 모든 선형 조합 $Z_1, Z_2, \cdots, Z_m$이 선형적으로 독립적이면(linearly independent) 제약 조건이 아무런 영향을 미치지 않고, 차원 축소가 이뤄지지 않게되며, 원래 예측 변수를 활용한 최소 제곱법을 수행하는 것과 같게 됩니다.

차원 축소 방법은 두 단계로 작동합니다. 먼저 변환된 예측 변수 $Z_1, Z_2,\dots, Z_m$을 구합니다. 그리고 예측 변수$Z_1, \dots, Z_m$을 사용하여 모형을 적합시킵니다.


이와같이 차원축소 방법은 원래 변수의 선형결합을 이용하며 주성분 분석과 부분 최소 제곱 그리고 충분 차원 축소에 대해서 다뤄보겠습니다.

### 주성분분석(Principal Componet Analysis)

주성분 분석은 다변량 변수 집합으로부터 저차원 특징 집합을 도출하는 일반적인 접근법으로 대표적인 비감독학습(unsupervised learning)
중 하나입니다. 주성분분석은 $n\times p$자료행렬 $X$을 $m$개의 주성분을 활용하여 $n\times m$차원으로 축약시킵니다. 또한 추출된 주성분들은 서로 상관관계가 없으며, 원래의 변수들의 특정 선형결합(linear combination)으로 표현됩니다. 

주성분 분석에서 새로 생성된 변수들은 중요도에 따라 내림차순으로 정렬됩니다. 즉, 첫번째 주성분은 관측치의 가장 큰 변동을 나타내며, 두번째 주성분은 첫번째 주성분과 상관관계가 없으면서(uncorrelated) 남아있는 변동 가운데 가장 많은 변동을 설명합니다. 이런식으로
$i$번째 주성분은 처음 $i-1$개의 주성분들과 상관관계가 없으면서 남아있는 변동 가운데 가장 많은 변동을 설명할 수 있도록 선택됩니다. 

주성분 분석의 일반적인 목표는 처음 몇 개의 주성분을 이용하여 원래 자료의 변동의 대부분을 설명하는 지 조사하는 데 있읍니다. 이 경우 처음 몇 개의 주성분을 이용하여 정보를 거의 손실하지 않고 자료를 요약할 수 있기 때문에 차원축소의 효과를 가집니다. 

예를 들어 여러 과목의 시험 성적에 대한 유용한 정보를 추출하는 문제를 생각해봅시다. 물론 각 학생들의 평균점수가 대표적인 선택이 될 수 있을 것입니다. 만약 시험 성적의 첫번째 주성분을 사용하면 어떻게 될까요? 첫번째 주성분은 학생들을 최대로 구분하는 인덱스를 제공하며 학생들의 표본 내에서 변화가 큰 점수일수록 높은 가중치를 갖게 됩니다. 첫번째 주성분이 너무 명확하다면, 두번째 혹은 그 이후의 구성요소들에 더 관심을 가질 수도 있습니다.

때로는 주성분을 구하는 것 그 자체가 목적이 될 수 있습니다. 대표적인 예가 다중회귀분석입니다. 관측치에 비해 설명변수의 수가 너무 많거나 설명변수들간에 상관관계가 매우 높은 경우에 차원 축소를 위해 원래 변수 대신 처음 몇 개의 주성분을 이용할 수 있습니다. 

$p$차원 자료 $\bx=(x_1, x_2, \cdots, x_p)'$가 공분산행렬 $\Sigma$를 갖고 $\Sigma$의 고유값(eigenvalue)이 $\lambda_1 \ge \lambda_2 \ge \cdots \lambda_p \ge 0$이라고 합시다. 다음과 같은 선형 결합을 생각해봅시다.
\begin{align*}
z_1 &= \ba_1'\bx = a_{11}x_1+a_{12}x_2+\cdots+a_{1p}x_p\\
z_2 &= \ba_2'\bx = a_{11}x_1+a_{22}x_2+\cdots+a_{2p}x_p\\
&\vdots\\
z_p &= \ba_p'\bx = a_{p1}x_1+a_{p2}x_2+\cdots+a_{pp}x_p\\
\end{align*}
그러면 $i, j = 1, 2, \cdots, p$에 대해
$Var(z_i)= \ba_i'\Sigma\ba_i$, $Cov(z_i, z_j) = \ba_i'\Sigma\ba_j$ 입니다. 주성분은 이와 같이 분산을 가장 크게 하는 형태로서 원래의 변수들의 선형결합으로 나타내집니다.

첫 번째 주성분은 $\ba_1'\ba_1 = 1$을 만족하면서 $Var(z_1)$을 최대로 하는 다음과 같은 선형결합입니다.
$$
z_1 = \ba_1'\bx = a_{11}x_1+a_{12}x_2+\cdots+a_{1p}x_p
$$

두 번째 주성분은 $\ba_2'\ba_2 = 1, \ba_2'\ba_1=0$을 만족하면서 $Var(z_2)$을 최대로 하는 다음과 같은 선형결합입니다.

$$
z_2 = \ba_2'\bx = a_{21}x_1+a_{22}x_2+\cdots+a_{2p}x_p
$$

이와 같은 방식으로 $i$번째 주성분은 $\ba_i'\ba_i=1$을 만족하고 $j
<i$에 대해 $\ba_i'\ba_j=0$을 만족하면서 $Var(z_i)$을 최대로 하는 다음과 같은 선형결합입니다. 

$$
z_i = \ba_i'\bx = a_{i1}x_1+a_{i2}x_2+\cdots+a_{ip}x_p
$$

\textbf{정리 1} $\Sigma$가 자료 $\bx'=[x_1, x_2, \cdots, x_p]$의 공분산 행렬이고, 고유값-고유벡터로 $(\lambda_1,\ba_1), (\lambda_2,\ba_2), \cdots, (\lambda_p, \ba_p)$를 갖는다고 합시다. 여기서 $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p \ge 0$을 만족합니다. 그러면 $j$번째 주성분은 다음과 같이 주어집니다.

$$
z_j = \ba_j'\bx = a_{j1}x_1+a_{j2}x_2+\cdots+a_{jp}x_p, \qquad j = 1, 2, \cdots, p
$$

또한 주성분은 $Var(z_j)=\ba_j'\Sigma\ba_j =\lambda_j$, $Cov(z_j,z_i)=\ba_j'\Sigma\ba_i$를 만족합니다.

\textbf{증명} $\Lambda=diag(\lambda_1, \lambda_2, \cdots, \lambda_p)$라고 하고, $\bA=[\ba_1,\ba_2, \cdots, \ba_p]$라고 합시다. 즉, $\Sigma=\bA\Lambda\bA'$입니다. 또한 $\Sigma^{1/2}=\bA\Lambda^{1/2}\bA', \bv=\bA'\bw$ 라고 합시다. 
그러면

$$
\frac{\bw'\Sigma\bw}{\bw'\bw}=\frac{\bw'\Sigma^{1/2}\Sigma^{1/2}\bw}{\bw'\bA\bA'\bw}=\frac{\bw'\bA\Lambda^{1/2}\bA'\bA\Sigma^{1/2}\bA\bw}{\bv'\bv}=\frac{\bv'\Lambda\bv}{\bv'\bv}=\frac{\sum_{i=1}^{p}\lambda_iv_i^2}{\sum_{i=1}^{p}v_i^2}\le \lambda_1 \frac{\sum_{i=1}^{p}v_i^2}{\sum_{i=1}^{p}v_i^2}=\lambda_1
$$
입니다. $\bw=\ba_1$이라고 하면 $\bv=\bP'\ba_1 =(1,0,0,\cdots,0)'$입니다. 따라서 

$$
\frac{\bv'\Lambda\bv}{\bv'\bv}=\frac{\lambda_1}{1}=\lambda_1 \quad \text{또는} \quad \frac{\ba_1'\Sigma\ba_1}{\ba_1'\ba_1}=\ba_1'\Sigma\ba_1=\lambda_1
$$

이 됩니다. 같은 방식으로 
$$\min_{\bw\neq 0}\frac{\bw'\Sigma\bw}{\bw'\bw}=\lambda_p$$
가 되며 $\bw=\ba_p$일 때 최솟값을 갖습니다. 이제 $\bw=\bP\bv=v_1\ba_1+v_2\ba_2+\cdots+v_p\ba_p$를 고려합시다. $\bw \perp \ba_1, \ba_2, \cdots, \ba_j$ 라는 것은 

$$
0 = \ba_i'\bw = v_1\ba_i'\ba_1+v_2\ba_i'\ba_2+\cdots+v_p\ba_i'\ba_p =v_i, i\le j
$$

을 의미합니다. 따라서 처음 $j$개의 고유벡터들과 수직인 $\bw$에 대해

$$
\frac{\bw'\Sigma\bw}{\bw'\bw}=\frac{\sum_{i=j+1}^{p}\lambda_iv_i^2}{\sum_{i=j+1}^p v_i^2}\le \lambda_{j+1}
$$
를 만족하며 $v_{j+1}=1, v_{j+2}=\cdots=v_p=0$을 선택하면 최대값 $\lambda_{j+1}$을 얻을 수 있습니다. 

한편 $Var(z_j)=\ba_{j}'\Sigma\ba_{j}=\lambda_j$이고
$\Sigma\ba_i = \lambda_i\ba_i$ 이므로, $Cov(z_j,z_i)=\ba_j'\Sigma\ba_i =\ba_j'\lambda_i\ba_i=\lambda_i\ba_j'\ba_i=0$을 만족함을 확인할 수 있습니다.

일반적으로 전체 변동의 80\% 이상을 설명하는 처음 주성분들을 선택하게 됩니다. 


\textbf{정리 2} $\Sigma$가 자료 $\bx'=[x_1, x_2, \cdots, x_p]$의 공분산 행렬이고, 고유값-고유벡터로 $(\lambda_1,\ba_1), (\lambda_2,\ba_2), \cdots, (\lambda_p, \ba_p)$를 갖는다고 합시다. 여기서 $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p \ge 0$을 만족합니다. $z_1=\ba_1'\bx, z_2=\ba_2'\bx, \cdots, z_p=\ba_p'\bx$ 를 주성분이라고 하면 다음이 성립합니다.
$$
\sigma_{11}+\sigma_{22}+\cdots+\sigma_{pp}=\sum_{i=1}^{p}Var(x_i)=\lambda_1+\lambda_2+\cdots+\lambda_p=\sum_{i=1}^{p}Var(z_i)
$$

\textbf{증명} $\sigma_{11}+\sigma_{22}+\cdots+\sigma_{pp}=tr(\Sigma)$임을 기억합시다. 그리고 $\Sigma=\bA\Lambda\bA'$이므로
$$
tr(\Sigma)=tr(\bA\Lambda\bA')=tr(\Lambda\bA'\bA)=tr(\Lambda)=\lambda_1+\lambda_2+\cdots+\lambda_p
$$
입니다.

즉, 전체 분산의 합은 고유값의 합과 같고 $j$번째 주성분이 설명하는 분산의 비율 $P_j$은
$$
P_j =\frac{\lambda_j}{\sum_{i=1}^{p}\lambda_i}
$$
가 됩니다. 따라서  전체 변동의 $P^*$만큼 설명하는 처음 $p^*$개의 주성분을 선택합니다. 즉,
$$
P^*=\frac{\sum_{i=1}^{p^*}\lambda_i}{\sum_{i=1}^{p}\lambda_i}
$$
을 만족합니다. 

일반적으로$\Sigma$는 알려져 있지 않습니다. 우리가 크기가 $n$인 표본 $x_1, x_2, \cdots, x_n$이 있다면 표본 분산행렬 $\bS$와 표본 상관행렬 $\bR$을 계산할 수 있습니다. 그래서 우리는 $\Sigma$대신 $\bS$를 이용하게 됩니다. $\bS$의 고유값-고유벡터 쌍 $(\hat{\lambda}_1,\hat{\ba}_1), (\hat{\lambda}_2,\hat{\ba}_2), \cdots, \hat{\lambda}_p,\hat{\ba}_p)$가 있다면 $j$번째 주성분은 다음과 같이 추정할 수 있습니다.

$$
\hat{z}_j =\hat{a}'_j\bx=\hat{a}_{j1}x_1+\hat{a}_{j2}x_2+\cdots+\hat{a}_{jp}x_p
$$
또한 $\hat{z}_j$의 표본 분산은 $\hat{\lambda}_j$가 되고 $j \neq i$일 때 $hat{z}_j$와 $\hat{z}_i$의 표본 공분산은 0이 됩니다. 또한 전체 분산의 합은
$$
\sum_{i=1}^p s_{ii}=\hat{\lambda}_1+\hat{\lambda}_2+\cdots+\hat{\lambda}_p
$$

가 됩니다. 

실제로는 표본 공분산행렬 $\bS$대신 표본 상관행렬 $\bR$에서 주성분을 추출하는 것이 더 일반적입니다. 설명변수 $x_1, x_2, \cdots, x_p$ 의 스케일이 서로 다른 경우가 많기 때문에, 표본 공분산행렬을 사용할 경우, 분산의 큰 변수만을 이용하여 축약될 가능성이 높기 때문입니다. 다시 말해서, 분산이 가장 큰 변수들 순서로 초기 주성분들이 이뤄지게 되는 문제가 있습니다. $\bR$에서 주성분을 추출하면 각 변수들이 동일하게 중요한 취급을 하기 때문에 이러한 문제를 해결할 수 있습니다.

$i$번째 개체 $x_i$의 주성분 점수는 다음과 같이 구합니다.
\begin{align*}
z_{i1}&=\hat{\ba}'_1(x_i-\bar{x})\\
z_{i2}&=\hat{\ba}'_2(x_i-\bar{x})\\
&\vdots \\
z_{ip}&=\hat{\ba}'_p(x_i-\bar{x})\\
\end{align*}
여기서 $\bar{x}$은 표본 평균입니다.

이렇게 변환된 변수들은 평균이 0이고 분산이 $\hat{\lambda}_1, \hat{\lambda}_2, \cdots, \hat{\lambda}_p$를 만족하게 됩니다. 

### 주성분회귀(Principal Componet Regression)

주성분 회귀 분석은 처음 $m$ 개의 주성분 $Z_1, Z_2, \dots, Z_m$을 예측변수로 사용하는 것을 말합니다. 주성분 회귀의 전제는 소수의 주성분이 예측 변수와 반응 사이의 관계뿐만 아니라 자료의 변동성의 대부분을 설명하기에 충분하다는 것입니다. 이러한 가정 하에서 $m <<p$개의 설명변수들 $Z_1, \dots, Z_m$만을 활용한 적합 모형이 원래의 $p$개의 설명변수 $X_1, \dots, X_p$를 이용한 적합 모형보다 더 좋을 수도 있습니다. 주성분을 적게 사용하면 편향이 증가하지만 분산을 줄고, 반대로 주성분을 많이 사용하면 편향은 감소하지만 분산은 증가한다는 점에서 편향=분산 트레이드오프(bias-variance trade-off)와 관련이 있습니다. 주성분 회귀는 예측 변수의 변동 및 반응과의 관계를 대부분 포착하기에 충분한 주성분이 적은 경우(m이 작은 경우)에서 더 잘 수행되는 경향이 있습니다. $m$이 $p$에 가까울수록 주성분 회귀는 원래 예측 변수에 적합한 최소 제곱 모형의 결과와 유사합니다.

회귀 분석에 사용되는 각 $m$개의 주성분은 모든 원래 예측 변수의 선형 조합이기 때문에 주성분 회귀는 특징 선택 방법이 아닙니다. 이러한 이유로 주성분 회귀는 라쏘 회귀 분석보다 능형 회귀 분석과 더 유사하다고 할 수 있습니다. 사용하는 주성분의 갯수 $m$의 선택을 위해 교차타당법(cross-validation)을 사용할 수 있습니다. 

주성분을 추출하기 전에 먼저 설명변수들을 표준화할 것을 권장합니다. 표준화된 설명변수를 사용하면 모든 변수가 동일한 척도에 있으므로 분산이 큰 예측 변수만이 주성분의 대부분을 지배하는 상황을 제한할 수 있습니다. 또한 표준화에 따라 추출되는 주성분이 달라질 수 있기 때문에 만약 변수가 모두 동일한 단위인 경우 표준화를 하지 않을 수도 있습니다.

### 부분 최소 제곱(Partial Least Squares)

주성분 회귀 분석과 달리 부분 최소 제곱은 반응 변수가 차원 축소 과정에 관여한다는 점에서 감독 학습 방법(supervised learning)입니다. 

부분 최소 제곱(PLS)은 원래 예측 변수의 선형 조합인 새로운 변수 $Z_1, \dots Z_m$을 식별한 다음 이러한 $m$개의 새 변수들을 사용하여 최소 제곱을 사용하여 선형 모형을 적합시킵니다. 주성분 회귀 분석과 달리 부분 최소 제곱은 반응 $Y$를 사용하여 원래 예측 변수와 비슷하면서 반응과 관련된 새로운 특징을 식별합니다.

첫 번째 부분 최소 제곱 성분을 구하기 전에 먼저 예측 변수를 표준화합니다. 다음으로, 각 $\phi_{j1}$ 계수의 값은 $Y$를 $X_j$로 단순 선형 회귀 분석하여 설정합니다. 이렇게 도출된 계수는 $Y$와 $X_j$의 상관계수에 비례하게 됩니다. 이러한 비례 관계 때문에 부분 최소 제곱은 계산할 때 반응과 가장 밀접한 관련이 있는 변수에 가장 높은 가중치를 두는 것을 알 수 있습니다. 이렇게 계산된 첫번째 부분 최소 제곱 성분은 다음과 같습니다.

$$
Z_1 =\sum_{j=1}^{p}\phi_{j1}X_j
$$

두 번째 부분 최소 제곱 방향을 식별하기 전에 먼저 $Z_1$에 대해 각 변수를 $Z_1$로 회귀하고 잔차를 계산합니다. 이러한 잔차는 첫 번째 부분 최소 제곱 방향으로 설명되지 않는 나머지 정보로 해석될 수 있습니다. 이 직교화된 데이터는 원래 데이터가 $Z_1$을 계산하는 데 사용된 것과 동일한 방식으로 $Z_2$를 계산하는 데 사용됩니다. 이 반복적 접근법은 여러 부분 최소 제곱 성분인 $Z_1,\dots, Z_m$을 식별하기 위해 $m$번 반복될 수 있습니다. 주성분 회귀 분석에서와 마찬가지로 부분 최소 제곱과 함께 사용되는 부분 최소 제곱 성분의 수 $m$는 일반적으로 교차 검증을 사용하여 선택합니다. 또한 부분 최소 제곱을 수행하기 전에 예측 변수와 반응변수를 일반적으로 표준화합니다.

### 고차원 데이터에 대한 고려 사항

회귀 및 분류에 사용되는 대부분의 통계 기법들은 $p <n$인 저차원 설정에서 고안되었습니다. 관측치보다 설명변수 또는 특징(feature)을 더 많이 포함하는 데이터를 고차원이라고 합니다. $p \ge n$인 경우, 최소 제곱 기법은 설명변수와 반응 사이에 실제로 관계가 있는지 여부에 관계없이 데이터에 완벽하게 적합한 계수 추정치를 계산합니다 ($R^2 = 1$). 따라서 고차원 환경에서는 최소 제곱 법을 사용할 수 없습니다. $C_p$, AIC, BIC 와 같은 통계량들 역시 $\sigma^2$을 추정하는 데 문제가 있으므로 고차원 환경에서 사용하는 데 제약이 있습니다.

고차원 설정에서는 오차 제곱합(Mean Squared Error; MSE), $p$-값, $R^2$ 또는 훈련자료에 적합된 다른 전통적인 측정값을 좋은 모형 적합의 증거로 사용해서는 안 됩니다. 독립적인 시험 자료 또는 교차 검증의 MSE 또는 $R^2$를 이용하여 모델 적합도를 평가하는 것을 추천합니다.

전향 단계적 선택 방법은 변수의 중요도에 따라 모형에 포함할 변수들을 조절할 수 있기 때문에 고차원 설정에서도 유용한 방법입니다. 정규화(regularization) 또는 수축(shringkage) 기법은 고차원 문제에서 핵심적인 역할을 수행할 수 있습니다. 능형 회귀 및 라소와 같은 기법들은 조율모수가 적절하게 선택된다면, 고차원 문제에서도 뛰어난 예측 성능을 보여줄 수 있습니다. 


# R 실습

### 주성분 회귀

`pls` 라이브러리에 있는 `pcr()`함수를 이용하여 주성분 회귀를 수행할 수 있습니다. 예전에도 다룬 바 있는 `Hitters` 데이터를 이용하여 타자들의 급여를 예측해봅시다. 먼저 이전과 같이 결측값을 제거하겠습니다.

```{r chunk1}
library(ISLR2)
library(pls)

head(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

`pcr()`함수의 구문은 `lm()`과 유사하게 작동합니다. 추가적인 옵션으로 `scale = TRUE`를 지정하면 주성분을 생성하기 이전에 각 변수들을 표준화하여 변수들이 측정된 척도가 주성분을 생성하는 데 미치는 영향을 제거할 수 있습니다. `validation ="CV"` 옵션을 지정하면 모형에 포함될 주성분의 수 $m$에 대한 10-단 교차 검증(10-fold cross-validation)을 수행한 결과를 알려줍니다. 적합 결과는 `summary()`를 통해 확인할 수 있습니다. 


```{r chunk2}
set.seed(2)
pcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE,
    validation = "CV")
summary(pcr.fit)
```
기본적으로`pcr()` 함수에서 제공하는 CV 값은 제곱근 평균 제곱 예측 오차(Root Mean Squared Error of Prediction)입니다. 일반적인 평균 제곱 예측오차를 알고싶다면 단순히 해당 값을 제곱하면 됩니다. 

`validationplot()`함수를 이용하여 교차 유효성 검사 점수를 그려볼 수 있습니다.

```{r chunk3}
validationplot(pcr.fit, val.type = "MSEP")
```

$m = 18$ 개의 성분을 사용할 때 가장 작은 교차 검증 오류가 발생하였습니다. 이는 PCR에서 모든 구성 요소를 사용할 때 치수 감소가 발생하지 않기 때문에 단순히 최소 제곱을 수행하는 $m = 19$ 보다는  적기는 합니다. 그러나 그래프에서 교차 검증 오차는 모형에 성분이 하나만 포함되어 있는 경우에도 매우 낮음을 알 수 있습니다. 따라서 소수의 성분만 사용하는 모형으로도 충분할 수 있습니다. 

`summary()` 함수는 또한 다른 수의 성분을 사용하여 설명 변수와 반응에 설명된 분산의 백분율을 제공합니다. 이는 $m$ 개의 주성분을 사용하여 포착된 설명 변수 또는 반응에 대한 정보의 양이라고 생각할 수 있습니다. 예를 들어, $m = 1$을 설정하면 설명 변수의 모든 분산 또는 정보의 38.31\%만 캡처됩니다. 반대로 $m = 5$를 사용하면 값이 84.29\%로 증가합니다. 이때 반응변수에 대해서는 총 변동의 40.63\%에서 44.90\%로 증가합니다. 이제 훈련자료에 대해 PCR을 수행하고 시험 자료로 성능을 평가해 봅시다.

```{r chunk4}
set.seed(1)
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
train <- sample (1: nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
pcr.fit <- pcr(Salary ~ ., data = Hitters, subset = train,
    scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
```

$m=5$일 때 교차 검증 오차가 가장 작다는 것을 확인할 수 있습니다. 이제 $m=5$개의 주성분을 사용한 주성분 회귀의 시험 오차를 구해봅시다. 

```{r chunk5}
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)
mean((pcr.pred - y.test)^2)
```

이 시험 MSE는 능형 회귀(142199.2) 및 라쏘(143673.6)를 사용하여 얻은 결과와 경쟁적입니다. 그러나 PCR의 특성상 최종 모형은 어떠한 종류의 변수 선택도 수행하지 않거나 심지어 직접 계수 추정치를 생성하지 않기 때문에 해석하기가 더 어렵습니다. 마지막으로, 교차 검증으로 식별된 구성 요소 수 $m= 5$를 사용하여 전체 데이터 세트에 PCR을 적합시킵니다. 

```{r chunk6}
pcr.fit <- pcr(y ~ x, scale = TRUE, ncomp = 5)
summary(pcr.fit)
```


### 부분 최소 제곱 회귀 

부분 최소 회귀는 위와 동일하게 `pls` 라이브러리의 `plsr()`함수를 이용하여 수행할 수 있습니다. 구문은 앞선 `pcr()`과 같습니다. 

```{r chunk7}
set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
```

$m=1$에서 교차 검증 오차가 가장 작았습니다. 이제 시험 자료로 MSE를 계산해봅시다. 


```{r chunk8}
pls.pred <- predict(pls.fit, x[test, ], ncomp = 1)
mean((pls.pred - y.test)^2)
```

능형 회귀 분석, 라쏘 및 PCR을 사용하여 얻은 시험 MSE와 비슷하지만 약간 높게 나타났습니다.  마지막으로, 교차 검증으로 식별된 성분 수 $m = 1$을 사용하여 전체 데이터 세트를 사용하여 PLS를 수행합니다.

```{r chunk9}
pls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE,
    ncomp = 1)
summary(pls.fit)
```

1-성분 PLS 적합치가 설명하는 급여의 분산 백분율인 43.05%는 최종 5-성분 모형 PCR 적합치인 44.90%를 사용하여 설명하는 것과 거의 동일합니다. 이는 PCR이 예측 변수에 설명된 분산의 양만 최대화하려고 하는 반면 PLS는 예측 변수와 반응의 분산을 설명하는 방향을 검색하기 때문입니다.
