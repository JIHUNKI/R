---
title: "통계학특강"
author: "강종경"
header-includes: \usepackage{setspace}\doublespacing
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: ["kotex","amsmath"]
    keep_tex: yes
  word_document: default
  html_document:
    df_print: paged
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.2
  kernelspec:
    display_name: R
    language: R
    name: ir
mainfont: NanumGothic
editor_options:
  markdown:
    wrap: sentence
fontsize: 12pt
---



\newcommand{\bx}{\boldmath{x}}
\newcommand{\bX}{\boldmath{X}}
\newcommand{\by}{\boldmath{y}}
\newcommand{\bY}{\boldmath{Y}}
\newcommand{\bH}{\boldmath{H}}
\newcommand{\bI}{\boldmath{I}}
\newcommand{\bK}{\boldmath{K}}

\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}

# 커널 기반 학습법(Kenrel-based Learning)

## 커널 능형 회귀(Kernel Ridge Regression)

커널 기반 학습을 수행하기 위해 `kernlab` 패키지를 이용합니다. 이 패키지는 서포트 벡터 기계, 커널 분위수 회귀 등 다양 커널 기반 학습법을 제공할 뿐만 아니라, 커널행렬의 생성에 필요한 여러 함수도 제공해줍니다. 

```{r kernlab}
library(kernlab)
```

커널 능형 회귀는 아래식을 최소화 하는 $\btheta$를 찾는 문제로 생각할 수 있습니다.

$$
\frac{1}{2}(\by-\bK\btheta)^T(\by-\bK\btheta)+\frac{\lambda}{2}\btheta^T\bK\btheta
$$

$\bK$가 양정치 행렬이라면 우리는 다음과 같은 닫힌해를 얻을 수 있습니다. 

$$\hat{\btheta} = (\bK+n\lambda\bI)^{-1}\by$$
`Hitters` 데이터를 이용하여 커널 능형회귀를 수행해봅시다. 먼저 데이터를 불러오고 결측치를 제거한뒤 훈련자료와 시험자료로 나누겠습니다.

```{r Hitters}
library(ISLR2)
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
set.seed(1)
train2 <- sample(1:nrow(x), nrow(x) / 2)
test2 <- (-train2)
y.train <- y[train2]
y.test <- y[test2]
```

여기서는 선형 커널, 그리고 가우시안 커널을 사용하여 커널 릿지회귀를 수행해보겠습니다.

커널행렬 $\bK$를 만들기 위해서 먼저 커널을 정해야 합니다. `kernlab`에서는 다음과 같은 커널들을 제공합니다. 

- The Gaussian RBF kernel $k(x,x') = \exp(-σ \|x - x'\|^2)$

- The Polynomial kernel $k(x,x') = (\text{scale} <x, x'> + \text{offset})^{\text{degree}}$

- The Linear kernel $k(x,x') = <x, x'>$

- The Hyperbolic tangent kernel $k(x, x') = \tanh(\text{scale} <x, x'> + \text{offset})$

- The Laplacian kernel $k(x,x') = \exp(-σ \|x - x'\|)$

- The Bessel kernel $k(x,x') = (- Bessel_{(ν+1)}^n σ \|x - x'\|^2)$

- The ANOVA RBF kernel $k(x,x') = ∑_{1≤q i_1 … < i_D ≤q N} ∏_{d=1}^D k(x_{id}, {x'}_{id})$ where $k(x,x)$ is a Gaussian RBF kernel.

- The Spline kernel $\prod_{d=1}^D 1 + x_i x_j + x_i x_j \min(x_i, x_j) - \frac{x_i + x_j}{2} \min(x_i,x_j)^2 + \frac{\min(x_i,x_j)^3}{3}$ 

우리는 선형 커널 `polydot(degree=1, scale=1, offset=1)`과  가우시안 커널 `rbfdot(sigma)`을 이용해볼겁니다. 가우시안 커널에서의 모수 `\sigma`도 모형의 성능에 영향을 많이 미칩니다. `kernlab`에서는 `sigest()`함수를 통해 일반적으로 모형이 잘 작동하게 하는 모수값을 추천해줍니다. 커널 행렬을 생성하기 전에 데이터는 표준화해주겠습니다.


```{r kernel}
l_ker   <- polydot(degree=1, scale=1, offset=1)
x.train <- scale(x[train2,])
x.test  <- scale(x[-train2,])

sigma1<- sigest(x.train)[2]
rbfker<-rbfdot(sigma = sigma1)
```

커널을 선정했으면 커널행렬을 만들어야 합니다. `kernelMatrix(kernel, X,X')`함수를 이용합니다. 마지막 인자는 생략할 수 있으며 이경우 `kernelMatrix(kernel, X, X)`형태를 입력한 것과 같습니다.

```{r kermat}
K_l<-kernelMatrix(l_ker,x.train)
K_r<-kernelMatrix(rbfker,x.train)
dim(K_l)
dim(K_r)
```

커널 릿지 회귀의 계수 $\btheta$의 추정치는 다음과 같이 구할 수 있습니다. 여기서는  $n\lambda= 1/n$을 이용해보겠습니다. 

```{r krr}
n1<-length(y.train)
theta1<- solve(K_l+1/n1*diag(n1))%*%y.train
theta2<- solve(K_r+1/n1*diag(n1))%*%y.train
```

$\hat{\by} = \bK\btheta$와 같이 계산됩니다.

```{r krr2}
yhat1<- K_l%*%theta1
yhat2<- K_r%*%theta2
```

실제 $y$와 얼마나 차이가 날까요?

```{r krr3}
par(mfrow=c(1,2))
plot(y.train, yhat1, main="Linear Kernel")
plot(y.train, yhat2, main="Gaussian Kernel")
```

가우시안 커널을 쓴 커널 능형회귀는 다소 과대적합한 경향이 보입니다. 이제 시험자료에 대해서 $y$값을 예측해봅시다. 시험자료에서의 예측값 역시 $\hat{y}_{test} =\bK_{test}\btheta$와 같이 계산되며 $\bK_{test}$행렬을 새로 만들어줘야 합니다.

```{r krr4}
K_lt <-kernelMatrix(l_ker,x.test,x.train)
K_rt <-kernelMatrix(rbfker,x.test,x.train)
y_test1<-K_lt%*%theta1
y_test2<-K_rt%*%theta2

mean((y.test-y_test1)^2)
mean((y.test-y_test2)^2)

par(mfrow=c(1,2))
plot(y.test, y_test1, main="Linear Kernel")
plot(y.test, y_test2, main="Gaussian Kernel")
```
두 값 모두 선형 방법론에서 추정한 시험 MSE보다 작은 값을 보여주는 것을 확인할 수 있습니다. 여기서는 하나의 $\lambda$값만을 고려하였지만, 실제로는 교차검증(cross-validation)을 통해 여러 후보 $\lambda$들 가운데 최적의 $\lambda$를 선택하는 것이 바람직합니다. 커널 방법을 통해 교차검증을 하기 위해서는 계속해서 커널 행렬을 새로 만들어줘야 하는 한계가 있습니다. 

```{r krrcv}
krr_cv<-function(X,Y, kernel =rbfdot, kernel.args=list(sigma = 1), lambda, fold=10, seed=1){
  ## X : should be scaled before
  n<-length(Y)
  kf <- do.call(kernel,kernel.args)
  set.seed(seed)
  folds <- sample(rep(1:fold, length = n))
cv.errors <- matrix(NA, fold, length(lambda), dimnames = list(NULL, round(lambda,6)))
  for(i in 1:fold){
      kmat <- kernelMatrix(kf,X[folds!=i,])
      n1<-dim(kmat)[1]
      theta<-sapply(1:length(lambda), function(j) solve(kmat+lambda[j]*diag(n1))%*%Y[folds!=i])
      kmat_t<-kernelMatrix(kf,X[folds==i,], X[folds!=i,])
      yhat<-kmat_t%*%theta
      cv.errors[i, ]<-sapply(1:length(lambda), function(j) mean((Y[folds==i]-yhat[,j])^2))
    }
  cv_errors<-colMeans(cv.errors)
  lambda.min<-lambda[which.min(cv_errors)]
  return(list(lambda.min = lambda.min, cv_errors=cv_errors))
}
```

후보 $\lambda$를 $2^{-10:5}$로 생성하고 교차검증을 통해 최적의 $\lambda$를 선택해봅시다. 

```{r krrcv1}
lambda_seq<-2^{-10:5}
cv_krr_l<-krr_cv(x.train, y.train, polydot, list(degree=1, scale=1, offset=1),lambda_seq)
cv_krr_r<-krr_cv(x.train, y.train, rbfdot, list(sigma =sigest(x.train)[2]),lambda_seq)
cv_krr_l$lambda.min
cv_krr_r$lambda.min
```

CV 함수처럼 krr 함수도 아래와 같이 만들어볼 수 있습니다. `predict` 함수도 함께 만들어봅시다. 

```{r krrftn}
krr<-function(X,Y, kernel =rbfdot, kernel.args=list(sigma = 1), lambda){
  ## X : should be scaled before
  n<-length(Y)
  kf <- do.call(kernel,kernel.args)
      kmat <- kernelMatrix(kf,X)
      n1<-dim(kmat)[1]
      theta<-solve(kmat+lambda*diag(n1))%*%Y
      model<-list(theta = theta, kernel =kf, X = X)
 'class<-'(model,"krr")     
}


predict.krr <- function(model, data){
  kmat <- kernelMatrix(model$kernel,data,model$X)
  ss<-c(kmat%*%model$theta)
  return(ss)
}

```

CV를 통해 최적화된 $\lambda$를 이용한 적합 결과는 아래와 같습니다. 적당히 $n\lambda =1/n$을 이용해 구한 값보다 시험 MSE 측면에서 개선된 결과를 가져다줌을 확인할 수 있습니다. 

```{r krr5}
krr_3<-krr(x.train,y.train,kernel=polydot, list(degree=1, scale=1, offset=1),lambda = 8)
krr_4<-krr(x.train,y.train,kernel=rbfdot, list(sigma=sigest(x.train)[2]),lambda = 0.5)
y_test3<-predict(krr_3,x.test)
y_test4<-predict(krr_4,x.test)
mean((y.test-y_test3)^2)
mean((y.test-y_test4)^2)
```

## 커널 서포트 벡터 기계

커널 서포트 벡터 기계는 `spam`데이터를 이용해보겠습니다. 이 데이터는 Hewlett-Packard Labs에서 수집되었으며 스팸 또는 스팸이 아닌 메일로 4601개의 메일로 구성되어있습니다. 설명변수로는 57개의 특정 문구 또는 문자에 대한 정보를 사용하였습니다. 

```{r spam}

## simple example using the spam data set
data(spam)

## create test and training set
index <- sample(1:dim(spam)[1])
spamtrain <- spam[index[1:floor(dim(spam)[1]/2)], ]
spamtest <- spam[index[((ceiling(dim(spam)[1]/2)) + 1):dim(spam)[1]], ]
```


커널 서포트 벡터 기계는 `kernlab` 패키지에 내장된 `ksvm()` 함수를 이용할 수 있습니다. `ksvm()`함수는 `lm()`과 같이 공식과 데이터를 입력하여 수행됩니다. 여기서는 `kernel = "rbfdot",을 이용하고 `sigma =0.05`를 이용하겠습니다. 벌점에 해당하는 `C`값은 `C=5`로 지정하였고, 10-fold CV 결과도 출력하게 하였습니다.

```{r ksvm1}
filter_5 <- ksvm(type~.,data=spamtrain,kernel="rbfdot",
               kpar=list(sigma=0.05),C=5,cross=10)
filter_5
```

`C=5`에서 훈련오분류율은 0.017, 교차검증오분류율은 0.084로 나타났습니다. 이번에는 `C=1`을 사용해볼까요?

```{r ksvm2}
filter_1 <- ksvm(type~.,data=spamtrain,kernel="rbfdot",
               kpar=list(sigma=0.05),C=1,cross=10)
filter_1
```

훈련오분류율이 0.035로 다소 올라갔으나, 교차검증오분류율은 0.080으로 오히려 내려갔습니다. `C=0.5`로 내려보겠습니다. 

```{r ksvm3}
filter_05 <- ksvm(type~.,data=spamtrain,kernel="rbfdot",
               kpar=list(sigma=0.05),C=.5,cross=10)
filter_05
```

훈련오분류율 뿐만아니라 교차검증오분류율도 올라갔기 때문에 우리는 `C=1`인 모형을 선택하겠습니다. 이제 시험오류율을 계산해봅시다. `spam`데이터는 58번째 열에 분류집단이 지정되어있으므로 이 값을 제외하고 `predict`함수를 적용해줍시다. 

```{r ksvm4}
mailtype <- predict(filter_1,spamtest[,-58])
(mail_table<-table(mailtype,spamtest[,58]))
1-sum(diag(mail_table))/sum(mail_table)

```

시험 오분류율은 0.076으로 약 92.4\%의 정확도로 스팸메일을 분류하는 기계를 만들었습니다. 일반적인 로지스틱 회귀를 했을때의 오분류율은 다음과 같습니다.

```{r glm}
spamtrain2<-spamtrain
spamtest2<-spamtest

spamtrain2$type<-as.numeric(spamtrain$type=="spam")
spamtest2$type<-as.numeric(spamtest$type=="spam")

filter_log<-glm(type~., data =spamtrain, family =binomial)
mailtype_log<-round(predict(filter_log,spamtest,type = "response"))
mail_table_log<-table(mailtype_log,spamtest[,58])
1-sum(diag(mail_table_log))/sum(mail_table_log)

```

## 커널 서포트 벡터 회귀

커널 서포트 벡터 회귀도 마찬가지로 `ksvm()`함수를 이용합니다. 여기서는 `C`대신 `epsilon`이 인자로 들어갑니다. 

```{r ksvr}
regm <- ksvm(x.train,y.train,epsilon=0.01,kpar="automatic",cross =10)
yhat_svr <-predict(regm, x.test)
mean((yhat_svr-y.test)^2)
```

`epsilon`값을 여러 값으로 바꿔서 해보세요. 너무 지나치게 크거나 작지만 않으면 커널 릿지 회귀보다도 시험 MSE측면에서 좋은 결과를 가져다 줍니다.


## 커널 분위수 회귀

커널 분위수 회귀는  `kqr()`함수를 이용합니다. `tau=0.5`가 기본값이며 이 경우 중위수 회귀를 수행하게 됩니다. 
```{r kqr}

kqr_1 <- kqr(x.train,y.train,kernel ="rbfdot", C=1, kpar=list(sigma=0.05))
yhat_kqr <-predict(kqr_1, x.test)
mean((yhat_kqr-y.test)^2)
```

`C`값을 여러 값으로 바꿔서 해보세요. 너무 지나치게 크거나 작지만 않으면 커널 릿지 회귀보다도 시험 MSE측면에서 좋은 결과를 가져다 줍니다.


