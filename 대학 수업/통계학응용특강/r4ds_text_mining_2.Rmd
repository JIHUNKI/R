---
title: "R for Data Science"
author: "Jongkyeong Kang"
date: "Kangwon National University"
output:
  html_document:
    toc: no
    df_print: paged
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: kotex
    toc: no
    keep_tex: yes
    pandoc_args: --variable=mathspec
fontsize: 9pt
mathspec: yes
subtitle: Text mining_2
theme: metropolis
---


```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache =TRUE)
img_path <- "img/"
```

```{r}
library(tidyverse) 
library(rvest)
```


## Extracting tables from a PDF

One of the datasets provided in `dslabs` shows scientific funding rates by gender in the Netherlands:

```{r}
library(dslabs)
data("research_funding_rates")
research_funding_rates |> 
  select("discipline", "success_rates_men", "success_rates_women")
```

<spreadsheet으로 제공되어지지 않아 오직 pdf문서의 테이블로 제공제졌다.>
The data comes from a paper published in the Proceedings of the National Academy of Science (PNAS), a widely read scientific journal. However, the data is not provided in a spreadsheet; it is in a table in a PDF document. Here is a screenshot of the table: 
 
```{r, echo=FALSE, out.width="60%"}
knitr::include_graphics(file.path(img_path, "pnas-table-s1.png"))
```

We try to wrangle the data using R. 

We start by downloading the pdf document, then importing into R:

```{r, eval=TRUE}
library("pdftools")
temp_file <- tempfile()  ## 랜덤하게 만들어진 temp
url2<-c("https://www.pnas.org/action/downloadSupplement?doi=10.1073%2Fpnas.1510159112&file=pnas.201510159SI.pdf")
download.file(url2, temp_file) ## temp_file이라는 경로를 이용하여 다운로드
txt <- pdf_text(temp_file)  ## url2을 txt에 각각에 페이지를 모두 저장
file.remove(temp_file) ## 경로로 이용했던 directory제거
```

`txt` is a character vector with an entry for each page. 

We keep the second page.

```{r, eval=TRUE}
raw_data_research_funding_rates_temp <- txt[2]  ##두번째 페이지에 대하여
```

위에 코드역시 'dslabs'페키지에 들어있는데이터이므로 생략 가능
The steps above can actually be skipped because we include this raw data in the `dslabs` package as well:

```{r}
data("raw_data_research_funding_rates")
```

 `raw_data_research_funding_rates` is a a long string 

 Each line on the page, including the table rows, are separated by the symbol for newline: `\n`. 
페이지에 각각의 table rows를 포함한 라인은 '\n'으로 구별되어있다.
그러므로 text라인의 리스트를 만들수 있다.
We therefore can create a list with the lines of the text 
 as elements as follows:

```{r}
tab <- str_split(raw_data_research_funding_rates_temp, "\n")
```

Note that `length(raw_data_research_funding_rates_temp)` is `1`. 
So we end up with a list with just one entry.
길이가 1이기 때문에 단지하나의 entry를 리스트하게 된다.
```{r}
tab <- tab[[1]]
```

col이름의 정보는 3번째 5번째 entry에 있다
The information for the column names is the third and fifth entries:

```{r}
the_names_1 <- tab[3]
the_names_2 <- tab[5]
```

The first of these rows looks like this:

```{r}
cat(the_names_1)
```
We want to create one vector with one name for each column. 
각각의 col에대해 하나의 이름을 가진 하나의 벡터를 만들기를 원한다.
- Remove the leading space 
- Remove anything following the comma. 
- Split when  there are 2 or more spaces (to avoid splitting `Success rates`)

```{r}
the_names_1 <- the_names_1 |>
  str_trim() |>                               ##공간 제거
  str_replace_all(",\\s.", "") |>             ##comma 다음에 어떠한것이든 제거
  str_split("\\s{2,}", simplify = TRUE)       ##2개이상의 공간을 분리
the_names_1 
```


Now we will look at `the_names_2`: 
```{r}
cat(the_names_2)
```

Here we want to trim the leading space and
then split by space as we did for the first line:

```{r}
the_names_2 <- the_names_2 |>
  str_trim() |>                       ## 뛰어쓰기 공간 제거
  str_split("\\s+", simplify = TRUE)  ##공백을 기준 단어로 split
the_names_2
```

We can then join these to generate one name for each column:
각 컬럼에 대해 하나의 이를을만들기 위해 join할 수 있다.

```{r}
tmp_names <- str_c(rep(the_names_1, each = 3), the_names_2[-1], sep = "_")
the_names <- c(the_names_2[1], tmp_names) |>
  str_to_lower() |>
  str_replace_all("\\s", "_")
the_names
```
이제 실제 데이터를 만들준비를하자 'tab'객체를 살펴봄으로써 우리는 이정보거 6~14줄에
있다는 것을 알아냈다. 'str_split'을 사용하여 목표를 달설하자

Now we are ready to get the actual data. By examining the `tab` object, we notice that the information is in lines 6 through 14. We can use `str_split` again to achieve our goal:

```{r}
new_research_funding_rates <- tab[8:16] |>
  str_trim() |>
  str_split("\\s{2,}", simplify = TRUE) |>
  data.frame() |>
  setNames(the_names) |>
  mutate_at(-1, parse_number)    ##parse_number 수치형이 아닌 값을 수치형으로
new_research_funding_rates |> as_tibble()
```
라이브러리 포함되어있는 데이터셋과 동일함을 확인할 수 있다.
We can see that the objects are identical:

```{r}
identical(research_funding_rates, new_research_funding_rates)
```
## JSON

- JSON(JavaScript Object Notation) is a standard data format.
- This JSON file looks more like the code you use to define a list. 

```{r, echo=FALSE}
library(jsonlite)
example <- data.frame(name= c("Miguel", "Sofia", "Aya", "Cheng"), student_id = 1:4, exam_1 = c(85, 94, 87, 90), exam_2 = c(86, 93, 88, 91))
json <- toJSON(example, pretty = TRUE) 
json
```

The file above actually represents a data frame.

To read it, we can use the function `fromJSON` from the `jsonlite` package. 
이것을 읽어내기 위해서 'fromJSON'함수를 사용한다.
Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. 

Here is an example providing information Nobel prize winners:

```{r}
nobel <- fromJSON("http://api.nobelprize.org/v1/prize.json")
nobel
```

This downloads a list. The first argument is a table with information about Nobel prize winners:

```{r}
filter(nobel$prize, category == "literature" & year == "1971") |> pull(laureates)
```

You can learn much more by examining tutorials and help files from the `jsonlite` package. 

## Case study: Trump tweets
hyperbolic : 과장된 assertion : 주장
During the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. On August 6, 2016, Todd Vaziri tweeted about Trump that "Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him)." 
Data scientist David Robinson conducted an analysis^[http://varianceexplained.org/r/trump-tweets/] to determine if data supported this assertion. Here, we go through David's analysis to learn some of the basics of text mining. 

We will use the following libraries:
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(scales)
```

David's provide the full data set at [https://www.thetrumparchive.com](https://www.thetrumparchive.com).
You can download it eather as [https://drive.google.com/file/d/1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6/view?usp=sharing](CSV file) or [https://drive.google.com/file/d/16wm-2NTKohhcA26w-kaWfhLIGwl_oX95/view?usp=sharing](JSON file). 

Here we focus on the JSON file and 
```{r, eval=FALSE}
file_names <- 'tweets_01-08-2021.json'

trump_tweets_temp <- jsonlite::fromJSON(file_names,simplifyDataFrame = TRUE) |>
  filter(isRetweet=="f" & !str_detect(text, '^"')) |>
  mutate(created_at = parse_date_time(date, orders = "Ymd HMS"))  |>  
  filter(year(created_at)<2018 & year(created_at)>2008) |>
  as_tibble()
```
편의를 위해 'dsalbs'패키지의 데이터를 불러오겠습니다.
For convenience, we include the result of the code above in the __dslabs__ package:

```{r}
library(dslabs)
data("trump_tweets")
```

You can see the data frame with information about the tweets by typing

```{r, eval=FALSE}
head(trump_tweets)
```

with the following variables included:

```{r}
names(trump_tweets)
```

The help file `?trump_tweets` provides details on what each variable represents. The tweets are represented by the `text` variable:

```{r}
cat(trump_tweets$text[11])
```
어떤 source의 장치로 구성되었는지
and the source variable tells us which device was used to compose and upload each tweet:

```{r}
trump_tweets |> count(source) |> arrange(desc(n)) 
```

We are interested in what happened during the campaign, so for this analysis we will focus on what was tweeted between the day Trump announced his campaign and election day. 캠패인과 선거기간 사이에 발생된 트윗에 초점을 두고 분석

We define the following table containing just the tweets from that time period. 
그 기간만을 포함한 테이블로 규제한다.
Note that we use `extract` to remove the `Twitter for` part of the source and filter out retweets.
android와iphone안에 있는 데이터

```{r}
campaign_tweets <- trump_tweets |> 
  extract(source, into = "source", regex="Twitter for (.*)") |>
  filter(source %in% c("Android", "iPhone") &
           created_at >= ymd("2015-06-17") & 
           created_at < ymd("2016-11-08")) |>
  filter(!is_retweet) |>
  arrange(created_at) |> 
  as_tibble()
campaign_tweets
```

We can now use data visualization to explore the possibility that two different groups were tweeting from these devices.
각가의 기기로부터 두그룹의 트윗하는 긍정성을 탐험하기위한 시각화를 할 수 있다.
For each tweet, we will extract the hour, East Coast time (EST), it was tweeted and then compute the proportion of tweets tweeted at each hour for each device:

```{r tweets-by-time-by-device}
campaign_tweets |>
  mutate(hour = hour(with_tz(created_at, "EST"))) |>
  count(source, hour) |>
  group_by(source) |>
  mutate(percent = n / sum(n)) |>
  ungroup() |>
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = percent_format()) +  ## ylab 에 퍼센트 단위에 맞춰줌
  labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
```


We notice a big peak for the Android in the early hours of the morning, between 6 and 8 AM. 안드로이드는 6~8시에 크게 나다타는것을 알 수 있다. 

There seems to be a clear difference in these patterns. 
이패턴에 분명한 차이가 보여진다.
We will therefore assume that two different entities are using these two devices.
우리는 두 다른 엔터티가 두개의 다른 기기에 사용되었음을 추측할 수 있다
We will now study how the tweets differ when we compare Android to iPhone.
이제 안드로이드와 아이폰을 비교하여 어떻게 두 트윗이 다른지 알아보자

## Text as data

The __tidytext__ package helps us convert free form text into a tidy table.
Having the data in this format greatly facilitates data visualization and the use of statistical techniques. 

```{r}
library(tidytext)
```

각가의 데이터 포인트로 고려하여 유닛으로 추론한다.
A `token` refers to a unit that we are considering to be a data point.
- ex) words, single characters, sentences, lines, regex, etc. 
스트링 벡터를 가지거나 토큰을 추출한다.
각각은 새로운 테이블의 row를 얻는다.
`unnest_tokens` will take a vector of strings and extract the tokens so that each one gets a row in the new table. Default token is "words". 

```{r}
poem <- c("Roses are red,", "Violets are blue,", 
          "Sugar is sweet,", "And so are you.")
example <- tibble(line = c(1, 2, 3, 4),
                      text = poem)
example
example |> unnest_tokens(word, text)
```

Now let's look at an example from the tweets.

We will look at tweet number 3008 because it will later permit us to illustrate a couple of points:
이것이 나중에 우리가 몇가지 요점을 설명할 수 있게 해주기 때문인다.

```{r}
i <- 3008
cat(campaign_tweets$text[i], "\n")
campaign_tweets[i,] |> 
  unnest_tokens(word, text) |>
  pull(word) 
```
이함수는 '#' '@'를 제거한다 
The function removes all the `#` and `@`. A _token_ in the context of 
트위터는 문장과 영어만이 내용이 아니기때문에
Twitter is not the same as in the context of spoken or written English. 
word대신에 tweets를 사용하여 패턴을포함하였다.
For this reason, instead of using the default, words, we use the `tweets` token includes patterns that start with @ and #:


```{r, message=FALSE, warning=FALSE}
campaign_tweets[i,] |> 
  unnest_tokens(word, text, token = "tweets") |>
  pull(word)
```

Another minor adjustment we want to make is to remove the links.
우리가 제거하고 싶은 링크부분은 제거
```{r, message=FALSE, warning=FALSE}
links <- "https://t.co/.+"
campaign_tweets[i,] |> 
  mutate(text = str_replace_all(text, links, ""))  |>
  unnest_tokens(word, text, token = "tweets") |>
  pull(word)
```

Now we are now ready to extract the words for all our tweets. 

```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets |> 
  mutate(text = str_replace_all(text, links, ""))  |>
  unnest_tokens(word, text, token = "tweets")
```


And we can now answer questions such as "what are the most commonly used words?":

```{r}
tweet_words |> 
  count(word) |>
  arrange(desc(n))
```

It is not surprising that these are the top words. 
빈도가 많은단어들을 살펴보면 별로 정보력이 없는 단어들이다
The top words are not informative. 

The _tidytext_ package has a database of these commonly used words,referred to as _stop words_, in text mining:

```{r}
stop_words ## the a 와 같은 일반적으로 많이 사용되는 단어들
## 일반적으로 많이 사용되는 _stop words_
```

If we filter out rows representing stop words with `filter(!word %in% stop_words$word)`:

```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets |> 
  mutate(text = str_replace_all(text, links, ""))  |>
  unnest_tokens(word, text, token = "tweets") |>
  filter(!word %in% stop_words$word ) 
```

we end up with a much more informative set of top 10 tweeted words:

```{r}
tweet_words |> 
  count(word) |>
  top_n(10, n) |>
  mutate(word = reorder(word, n)) |>
  arrange(desc(n))
```
결과에서 원하지 않은 두가지 character가 보여진다 이것을 제거하자
- 단지 숫자
- `'`로 시작하는 인용구
Some exploration of the resulting words (not shown here) reveals a couple of unwanted characteristics in our tokens. 
 - Just numbers (years, for example).
 - quote that start with `'`

```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets |> 
  mutate(text = str_replace_all(text, links, ""))  |>
  unnest_tokens(word, text, token = "tweets") |>
  filter(!word %in% stop_words$word &
           !str_detect(word, "^\\d+$")) |>
  mutate(word = str_replace(word, "^'", ""))
```

Now we can start exploring which words are more common when comparing Android to iPhone. 

For each word, we want to know if it is more likely to come from an Android tweet or an iPhone tweet.
- Odds ratio are a summary statistic useful for quantifying these differences.
- Here we will have many proportions that are 0, so we use the 0.5 correction.

```{r}
android_iphone_or <- tweet_words |>
  count(word, source) |>
  pivot_wider(names_from = "source", values_from = "n", values_fill = 0) |>
  mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) / 
           ( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
```

Here are the highest odds ratios for Android

```{r}
android_iphone_or |> arrange(desc(or))
```

and the top for iPhone:
```{r}
android_iphone_or |> arrange(or)
```  
작은 빈도의 단어들을 고려해볼때 우리는 이와 같은 전반전인 빈도에 기초하여 필터를 부과할 수 있다.
Given that several of these words are overall low frequency words, we can impose a filter based on the total frequency like this:

```{r}
android_iphone_or |> filter(Android+iPhone > 100) |>
  arrange(desc(or))
android_iphone_or |> filter(Android+iPhone > 100) |>
  arrange(or)
```
우리는 이미 한장치에서 다른 장치보다 더많은 트윗되는 단어 유형
에서 어느 정도 패턴을 확인 할 수 있다.

그러나 우리는 특정 단어가 아니라 어조에 관심이 있다.
We already see somewhat of a pattern in the types of words that are being tweeted more from one device versus the other. 

However, we are not interested in specific words but rather in the tone. 

Vaziri's assertion is that the Android tweets are more **hyperbolic**. So how can we check this with data? 
안드로이드 트윗이 더 과장 되어 있다고 주장하였다. 그렇다면
데이터로 이것을 어떻게 확인할 수 있을까?


## Sentiment analysis 감정분석

In sentiment analysis, we assign a word to one or more "sentiments". 
하나이상의 '감정'에 단어를 할당
Although this approach will miss context-dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.
이접근 방식은 풍자와 같은 상황에서 따른 감정을 놓치지만 많은 수의
단어에 대해 수행될 때 요약은 통찰력을 제공할 수 있다.
The first step in sentiment analysis is to assign a sentiment to each word.
각 단어에 감정을 할당

```{r, message=FALSE, warning=FALSE}
library(tidytext)
library(textdata)
```

The `bing` lexicon divides words into `positive` and `negative` sentiments. 긍정 부정 단어로 나눠준다
 We can see this using the _tidytext_ function `get_sentiments`:

```{r}
get_sentiments("bing")
```

The `AFINN` lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.
가장 부정적인 단어에 -5 가장 긍정적인 단어에 5
```{r}
get_sentiments("afinn")
```

The `loughran` and `nrc` lexicons provide several different sentiments.  두가지 사전은 여러가지 다른 감정을 제공

```{r}
get_sentiments("loughran") |> count(sentiment)
```

```{r}
get_sentiments("nrc") |> count(sentiment)
```

우리 분석에서는 nrc사전을 이용하여 감정을 분석하겠습니다
For our analysis, we are interested in exploring the different sentiments of each tweet so we will use the `nrc` lexicon:

```{r}
nrc <- get_sentiments("nrc") |>
  dplyr::select(word, sentiment)

```
감정과 트윗에 단어를 관련된 단어와 inner_join하여 관련된 단어만 유지
We can combine the words and sentiments using `inner_join`, which will only keep words associated with a sentiment. Here are 10 random words extracted from the tweets:


```{r}
tweet_words |> inner_join(nrc, by = "word") %>%
  dplyr::select(source, word, sentiment) |> 
  sample_n(5)
```

Now we are ready to perform a quantitative analysis comparing Android and iPhone by comparing the sentiments of the tweets posted from each device.

Here we could perform a tweet-by-tweet analysis, assigning a sentiment to each tweet. 
이제 각 기기에서 게시 된 트윗의 감정을 비교하여 Android와 iPhone을 비교하는 정량 분석을 수행 할 준비가되었습니다.

여기서 트윗별 분석을 수행하여 각 트윗에 감정을 할당할 수 있습니다. 
However, this will be challenging since each tweet will have several sentiments attached to it, one for each word appearing in the lexicon.

For illustrative purposes, we will perform a much simpler analysis: we will count and compare the frequencies of each sentiment appearing in each device.

그러나 각 트윗에는 사전에 나타나는 각 단어에 대해 하나씩 여러 감정이 첨부되어 있기 때문에 이것은 어려울 것입니다.

설명을 위해, 우리는 훨씬 간단한 분석을 수행 할 것입니다 : 우리는 각 장치에 나타나는 각 감정의 빈도를 계산하고 비교할 것입니다.


```{r}
sentiment_counts <- tweet_words |>
  left_join(nrc, by = "word") |>
  count(source, sentiment) |>
  pivot_wider(names_from = "source", values_from = "n") |>
  mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts
```

For each sentiment, we can compute the odds of being in the device: proportion of words with sentiment versus proportion of words without, and then compute the odds ratio comparing the two devices.
각 감정에 대해 장치에 있을 확률(감정이 있는 단어의 비율 대 없는 단어의 비율)을 계산한 다음 두 장치를 비교하는 승산비를 계산할 수 있습니다.


```{r}
sentiment_counts |>
  mutate(Android = Android / (sum(Android) - Android) , 
         iPhone = iPhone / (sum(iPhone) - iPhone), 
         or = Android/iPhone) |>
  arrange(desc(or))
```

The largest three sentiments are disgust, anger, and negative!

We can calculate the confidence interval to see if these odds ratios are statistically significant.

가장 큰 세 가지 감정은 혐오감, 분노, 부정적입니다!

신뢰 구간을 계산하여 이러한 승산비가 통계적으로 유의한지 확인할 수 있습니다.

```{r}
library(broom)
log_or <- sentiment_counts |>
  mutate(log_or = log((Android / (sum(Android) - Android)) / 
      (iPhone / (sum(iPhone) - iPhone))),
          se = sqrt(1/Android + 1/(sum(Android) - Android) + 
                      1/iPhone + 1/(sum(iPhone) - iPhone)),
          conf.low = log_or - qnorm(0.975)*se,
          conf.high = log_or + qnorm(0.975)*se) |>
  arrange(desc(log_or))
  
log_or
```

A graphical visualization shows some sentiments that are clearly overrepresented:

```{r tweets-log-odds-ratio}
log_or |>
  mutate(sentiment = reorder(sentiment, log_or)) |>
  ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point(aes(sentiment, log_or)) +
  ylab("Log odds ratio for association between Android and sentiment") +
  coord_flip() 
```

We see that the disgust, anger, negative, sadness, and fear sentiments are statistically significantly associated with the Android, which is in agreement with the original claim about hyperbolic tweets.
우리는 혐오감, 분노, 부정적, 슬픔 및 두려움 감정이 통계적으로 Android와 유의미하게 관련되어 있음을 알 수 있으며, 이는 쌍곡선 트윗에 대한 원래 주장과 일치합니다.

If we are interested in exploring which specific words are driving these differences, we can refer  back to our `android_iphone_or` object:
어떤 특정 단어가 이러한 차이를 주도하는지 탐구하는 데 관심이 있다면 'android_iphone_or' 개체를 다시 참조할 수 있습니다.
```{r}
android_iphone_or |> inner_join(nrc, by = "word") |>
  filter(sentiment == "disgust" & Android + iPhone > 10) |>
  arrange(desc(or))
``` 

and we can make a graph:

```{r log-odds-by-word, out.width="100%"}
android_iphone_or |> inner_join(nrc, by = "word") |>
  mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) |>
  mutate(log_or = log(or)) |>
  filter(Android + iPhone > 10 & abs(log_or)>1) |>
  mutate(word = reorder(word, log_or)) |>
  ggplot(aes(word, log_or, fill = log_or < 0)) +
  facet_wrap(~sentiment, scales = "free_x", nrow = 2) + 
  geom_bar(stat="identity", show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```
