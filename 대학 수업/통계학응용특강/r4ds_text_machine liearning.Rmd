---
title: "R for Data Science"
author: "Jongkyeong Kang"
date: "Kangwon National University"
output:
  html_document:
    toc: no
    df_print: paged
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: kotex
    toc: no
    keep_tex: yes
    pandoc_args: --variable=mathspec
fontsize: 9pt
mathspec: yes
subtitle: Machine Learning
theme: metropolis
---

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache =TRUE)
img_path <- "img/"
```

# Linear Regression
The `ISLR2` library contains the `Boston`  data set, which
records `medv` (median house value) for $506$ census tracts in Boston. We will seek to predict `medv` using $12$ predictors such as `rmvar` (average number of  rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status).

In order to fit a linear regression model using least squares, we  use the `lm()` function. 

The syntax {\R{lm(y $\sim$ x1 + x2 + x3)}} is used to fit a model with three predictors, `x1`, `x2`, and `x3`.

The `summary()` function now outputs the regression coefficients for all the predictors.

```{r }
library(ISLR2)

lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

To fit a model with all predictors, we just type `.` for predictors. 

```{r }
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
```

The `vif()` function, part of the `car` package, can be used to compute variance inflation factors. 

```{r }
library(car)
vif(lm.fit)
```
 The following syntax results in a regression using all predictors except `age`.

```{r }
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
```

Alternatively, the `update()` function can be used.

```{r }
lm.fit1 <- update(lm.fit, ~ . - age)
```


## Interaction Terms

The syntax `lstat:black` tells `R` to include an interaction term between `lstat` and `black`.

The syntax `lstat * age` simultaneously includes `lstat`, `age`, and the interaction term `lstat`$\times$`age` as predictors; it is a shorthand for `lstat + age + lstat:age`.

```{r }
summary(lm(medv ~ lstat * age, data = Boston))
```


## Non-linear Transformations of the Predictors

 For instance, given a predictor $X$, we can create a predictor $X^2$ using  `I(X^2)`. 

```{r }
lm.fit2 <- lm(medv ~ lstat + I(lstat^2), data = Boston)
summary(lm.fit2)
```

We use the `anova()` function  to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{r }
lm.fit <- lm(medv ~ lstat, data = Boston)
anova(lm.fit, lm.fit2)
```
The following command produces a fifth-order polynomial fit:

```{r }
lm.fit5 <- lm(medv ~ poly(lstat, 5), data = Boston)
summary(lm.fit5)
```

By default, the `poly()` function orthogonalizes the predictor.
 
In order to obtain the raw polynomials from the `poly()` function,  the argument `raw = TRUE` must be used.

Other forms of conversion, such as log conversion, are also possible.

```{r }
summary(lm(medv ~ log(rm), data = Boston))
```


## Qualitative Predictors

We will now examine the `Carseats` data, which is part of the
`ISLR2` library.

We will  attempt to predict `Sales` (child car seat sales) in $400$ locations based on a number of predictors.

```{r }
head(Carseats)
```

The predictor `shelveloc` takes on three possible values:  *Bad*, *Medium*, and *Good*. 
Given a qualitative variable such as `shelveloc`, `R` generates dummy variables automatically.

Below we fit a multiple regression model that includes some interaction terms.

```{r }
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, 
    data = Carseats)
summary(lm.fit)
```

The `contrasts()` function returns the coding that `R` uses for the dummy variables.

```{r }
attach(Carseats)
contrasts(ShelveLoc)
```

Use `?contrasts` to learn about other contrasts, and how to set them.


# Classification Methods



## The Stock Market Data

`Smarket` data consists of percentage returns for the S\&P 500 stock index over $1,250$~days, from the beginning of 2001 until the end of 2005.

For each date, we have recorded the percentage returns for each of the five previous trading days, `lagone` through `lagfive`.

We have also recorded `volume` (the number of shares traded on the previous day, in billions), `Today` (the percentage return on the date in question)  and `direction` (whether the market was `Up` or `Down` on this date). 

Our goal is to predict `direction` (a qualitative response) using the other features.

We will first create a vector corresponding to the observations from 2001 through 2004 for training data.

```{r }
library(ISLR2)
names(Smarket)
dim(Smarket)
summary(Smarket)
attach(Smarket)
train <- (Year < 2005)
Smarket.2005 <- Smarket[!train, ]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]
```

## Logistic Regression

The `glm()` function  can be used to fit many types of generalized linear models , including logistic regression.

We must pass in the argument `family = binomial` in order to tell `R` to run a logistic regression.

Furthermore,  using the `subset` argument in order to use only the 
 subset of the observations, i.e. training data. 

```{r }
glm.fits <- glm(
    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
    data = Smarket, family = binomial, subset = train
  )
summary(glm.fits)
```

 We use the `coef()` function in order to access just the coefficients for this fitted model.
 
 We can also use the `summary()` function to access  particular aspects of the fitted model, such as the $p$-values for the coefficients.

```{r }
coef(glm.fits)
summary(glm.fits)$coef
summary(glm.fits)$coef[, 4]  ## p-value col 
```



The `predict()` function can be used to predict the probability that the market will go up, given values of the predictors. 

The `type = "response"` option tells `R` to output probabilities of the form $P(Y=1|X)$, as opposed to other information such as the logit. 

 `contrasts()` function indicates that `R` has created a dummy variable with a 1 for `Up`.

```{r }
glm.probs <- predict(glm.fits, Smarket.2005,
    type = "response")
glm.probs[1:10]
contrasts(Smarket$Direction)
```

In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, `Up` or `Down`.

The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than $0.5$.

```{r }
glm.pred.1<-ifelse(glm.probs>.5,"Up","Down")
glm.pred.1<-as.factor(glm.pred.1)

glm.pred <- rep("Down", length(Direction.2005))
glm.pred[glm.probs > .5] <- "Up"
glm.pred<-as.factor(glm.pred)
```

`table()` function can be used to produce a confusion matrix  in order to determine how many observations were correctly or incorrectly classified. 

It is more convenient to use `confusionMatrix()` function provided by the `caret` package.


```{r }
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
library(caret)
confusionMatrix(glm.pred,Direction.2005)
```

## Linear Discriminant Analysis

We fit an LDA model using the  `lda()` function, which is part of the `MASS` library.

```{r }
library(MASS)
lda.fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket,
    subset = train)
lda.fit
plot(lda.fit)
```

  If $-0.642\times `lagone` - 0.514 \times `lagtwo`$ is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.

  The `plot()` function produces plots of the *linear discriminants*, obtained by computing $-0.642\times `lagone` - 0.514 \times `lagtwo`$ for each of the training observations.


The `predict()` function returns a list with three elements. 
 -  `class`: contains LDA's predictions about the movement of the market. 
 - `posterior`: a matrix whose $k$th column contains the posterior probability that the corresponding observation belongs to the $k$th class.
 -`x`: contains the linear discriminants, described earlier.

```{r }
plot(lda.fit)
lda.pred <- predict(lda.fit, Smarket.2005)
names(lda.pred)
lda.pred$class
lda.pred$posterior
lda.pred$class;lda.pred$x
```

The LDA and logistic regression predictions are almost identical.

```{r }
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
```

## Quadratic Discriminant Analysis

QDA is  implemented in `R` using the `qda()` function, which is also part of the `MASS` library. 

```{r }
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket,
    subset = train)
qda.fit
```

The `predict()` function works in exactly the same fashion as for LDA.

```{r }
qda.class <- predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
mean(qda.class == Direction.2005)
```

## Naive Bayes

Naive Bayes is implemented in `R` using the `naiveBayes()` function, which is part of the `e1071` library.

By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. 

```{r }
library(e1071)
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,
    subset = train)
nb.fit
```

The output  contains the estimated mean and standard deviation for each variable in each class. For example, the mean for `lagone` is $0.0428$ for  `Direction=Down`, and the standard deviation is $1.23$. 

The `predict()` function is straightforward.

```{r }
nb.class <- predict(nb.fit, Smarket.2005)
table(nb.class, Direction.2005)
mean(nb.class == Direction.2005)
```

The `predict()` function can also generate estimates of the probability that each observation belongs to a particular class.

```{r }
nb.preds <- predict(nb.fit, Smarket.2005, type = "raw")
nb.preds[1:5, ];head(nb.class)
```


## $K$-Nearest Neighbors

Naive Bayes is implemented in `R` using the `naiveBayes()` function, which is part of the `e1071` library.

KNN is implemented in `R` using the `knn()` function, which is part of the `class` library. 

The function requires four inputs.

* A matrix containing the predictors associated with the training data, labeled `train.X` below.
* A matrix containing the predictors associated with the data for which we wish to make predictions, labeled `test.X` below.
* A vector containing the class labels for the training observations, labeled `train.Direction` below.
* A value for $K$, the number of nearest neighbors to be used by the classifier.

```{r }
library(class)
train.X <- cbind(Lag1, Lag2)[train, ]
test.X <- cbind(Lag1, Lag2)[!train, ]
train.Direction <- Direction[train]
```

We set a random seed before we apply `knn()` because if several observations are tied as nearest neighbors, then `R` will randomly break the tie.

```{r }
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.2005)
(83 + 43) / 252
confusionMatrix(knn.pred, Direction.2005)
```


# Regression splines

In order to fit regression splines in `R`, we use the `splines` library.

 `bs()` function generates the entire matrix of basis functions for splines with the specified set of knots. By default, cubic splines are produced.
 
 Here we analyze `Wage` data which contains wage and other data for a group of 3000 male workers in the Mid-Atlantic region. 
 
Following codes fit `wage` to `age` using a regression spline with  pre-specified knots at ages $25$, $40$, and $60$. 

```{r }
library(splines)
attach(Wage)
agelims <- range(age)
age.grid <- seq(from = agelims[1], to = agelims[2])
fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
pred <- predict(fit, newdata = list(age = age.grid), se = T)
plot(age, wage, col = "gray")
lines(age.grid, pred$fit, lwd = 2)
lines(age.grid, pred$fit + 2 * pred$se, lty = "dashed")
lines(age.grid, pred$fit - 2 * pred$se, lty = "dashed")
```

 We could also use the `df` option to produce a spline with knots at uniform quantiles of the data.

```{r }
dim(bs(age, knots = c(25, 40, 60)))
dim(bs(age, df = 6))
attr(bs(age, df = 6), "knots")
```

In order to instead fit a natural spline, we use the `ns()` function. 

```{r }
fit2 <- lm(wage ~ ns(age, df = 4), data = Wage)
pred2 <- predict(fit2, newdata = list(age = age.grid),
     se = T)
plot(age, wage, col = "gray")
lines(age.grid, pred2$fit, col = "red", lwd = 2)
```

  In order to fit a smoothing spline, we use the `smooth.spline()` function.

```{r }
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Smoothing Spline")
fit <- smooth.spline(age, wage, df = 16)
fit2 <- smooth.spline(age, wage, cv = TRUE)
fit2$df
lines(fit, col = "red", lwd = 2)
lines(fit2, col = "blue", lwd = 2)
legend("topright", legend = c("16 DF", "6.8 DF"),
    col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
```
In order to perform local regression, we use the `loess()` function. 

```{r }
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Local Regression")
fit <- loess(wage ~ age, span = .2, data = Wage)
fit2 <- loess(wage ~ age, span = .5, data = Wage)
lines(age.grid, predict(fit, data.frame(age = age.grid)),
    col = "red", lwd = 2)
lines(age.grid, predict(fit2, data.frame(age = age.grid)),
    col = "blue", lwd = 2)
legend("topright", legend = c("Span = 0.2", "Span = 0.5"),
    col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
```

## Generalized Additive Models


We now fit a GAM to predict `wage` using natural spline functions of `lyear` and `age`,  treating `education` as a qualitative predictor.

```{r }
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education,
    data = Wage)
summary(gam1)
```


In order to fit using smoothing splines  we will need to use the `gam` library in `R`.

The `s()` function, which is part of the `gam` library, is used to indicate that we would like to use a smoothing spline.

```{r }
library(gam)
gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education,
    data = Wage)
```


`plot()` (in fact, `plot.Gam()`) focuses on terms (main-effects), and produces a suitable plot for terms of different types:

```{r }
par(mfrow = c(1, 3))
plot(gam.m3, se = TRUE, col = "blue")
plot.Gam(gam.m3,se=T,col="blue")
```

 We can perform a series of ANOVA tests in order to determine which of these three models is best:  a GAM that excludes `lyear` ($\mathcal{M}_1$),  a GAM that uses a linear function of `lyear` ($\mathcal{M}_2$), or  a GAM that uses a spline function
of `lyear` ($\mathcal{M}_3$).

```{r }
gam.m1 <- gam(wage ~ s(age, 5) + education, data = Wage)
gam.m2 <- gam(wage ~ year + s(age, 5) + education,
    data = Wage)
anova(gam.m1, gam.m2, gam.m3, test = "F")
```


The `summary()` function produces a summary of the gam fit.

```{r }
summary(gam.m3)
```

We can make predictions using the `predict()` method for the class `Gam`. 

```{r }
preds <- predict(gam.m2, newdata = Wage)
```

We can also use local regression fits as building blocks in a GAM, using the `lo()` function.

```{r }
gam.lo <- gam(
    wage ~ s(year, df = 4) + lo(age, span = 0.7) + education,
    data = Wage
  )
plot.Gam(gam.lo, se = TRUE, col = "green")
```

We can also use the `lo()` function to create interactions before calling the `gam()` function. 

```{r }
gam.lo.i <- gam(wage ~ lo(year, age, span = 0.5) + education,
    data = Wage)
```

In order to fit a logistic regression GAM, we once again use the `I()` function in constructing the binary response variable, and set `family=binomial`.

```{r }
gam.lr.s <- gam(
    I(wage > 250) ~ year + s(age, df = 5) + education,
    family = binomial, data = Wage,
    subset = (education != "1. < HS Grad")
  )
plot(gam.lr.s, se = T, col = "green")
```


# Decision Trees

## Fitting Classification Trees


The `tree` library is used to construct classification and regression trees.

```{r }
library(tree)
```

We first use classification trees  to analyze the `Carseats` data set.

In these data, `Sales` is a continuous variable, and so we begin by recoding it  as a binary variable. 

We use the `ifelse()` function to create a variable, called `High`, which takes on a value of `Yes` if the `Sales` variable exceeds $8$, and takes on a value of `No` otherwise.

```{r }
library(ISLR2)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
```

Finally, we use the `data.frame()` function to merge `High` with the rest of the `Carseats` data.

```{r }
Carseats <- data.frame(Carseats, High)
```

We now  use the `tree()` function to fit a classification tree in order to predict `High` using all variables but `Sales`.

```{r }
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats,
    subset = train)
tree.pred <- predict(tree.carseats, Carseats.test,
    type = "class")
confusionMatrix(tree.pred, High.test)
```

The `summary()` function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.

```{r }
summary(tree.carseats)
```

We see that the training error rate is  $9\%$.

One of the most attractive properties of trees is that they can be  graphically displayed. 

We use the `plot()` function to display the tree structure, and the `text()` function to display the node labels. 

The argument `pretty = 0` instructs `R` to include the category names for any qualitative predictors, rather than simply displaying a letter for each category.

```{r }
plot(tree.carseats)
text(tree.carseats,pretty=0)
```

The function `cv.tree()` performs cross-validation in order to  determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration.


```{r }
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
```

Despite its name, `dev` corresponds to the number of
cross-validation errors. 
We plot the  error rate as a function of both `size` and `k`.

```{r }
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

We now apply the  `prune.misclass()`  function in order to prune the tree to obtain the nine-node tree.

```{r }
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

How well does this pruned tree perform on the test data set? Once again, we apply the `predict()` function.

```{r }
tree.pred <- predict(prune.carseats, Carseats.test,
    type = "class")
confusionMatrix(tree.pred, High.test)
```

## Fitting Regression Trees

Here we fit a regression tree to the `Boston`  data set. First, we create a training set, and fit the tree to the training data.

```{r }
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```

Notice that the output of `summary()` indicates that only four of the variables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. We now plot the tree.

```{r }
plot(tree.boston)
text(tree.boston, pretty = 0)
```

Now we use the `cv.tree()` function to see whether pruning the tree will improve performance.

```{r }
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

If we wish to prune the tree, we could do so as follows, using the `prune.tree()` function:

```{r }
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

In keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.

```{r }
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
```

## Bagging and Random Forests


Here we apply bagging and random forests to the `Boston` data, using the `randomForest` package in `R`.

Bagging is simply a special case of a random forest with $m=p$. 

```{r }
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 12, importance = TRUE)
bag.boston
```

How well does this bagged model perform on the test set?

```{r }
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```

We could change the number of trees grown by `randomForest()` using the `ntree` argument:

```{r }
bag.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```


By default, `randomForest()` uses $p/3$ variables when building a random forest of regression trees, and $\sqrt{p}$ variables when building a random forest of classification trees. Here we use `mtry = 6`.

```{r }
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```

Using the `importance()` function, we can view the importance of each variable.

```{r }
importance(rf.boston)
```

The first is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is permuted. 

The second is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees. 

Plots of these importance measures can be produced using the `varImpPlot()` function.

```{r }
varImpPlot(rf.boston)
```
## Boosting

We use the `gbm` package, and within it the `gbm()` function, to fit boosted regression trees to the `Boston` data set.

We run `gbm()` with the option `distribution = "gaussian"` since this is a regression problem; if it were a binary classification problem, we would use `distribution = "bernoulli"`.

The argument `n.trees = 5000` indicates that we want $5000$ trees, and the option `interaction.depth = 4` limits the depth of each tree.

```{r }
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
    distribution = "gaussian", n.trees = 5000,
    interaction.depth = 4)
```

The `summary()` function produces a relative influence plot and also outputs the relative influence statistics.

```{r }
summary(boost.boston)
```

We can also produce *partial dependence plots* for these two variables. These plots illustrate the marginal effect of the selected variables on the response after *integrating* out the other variables.

```{r }
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```
We now use the boosted model to predict `medv` on the test set:

```{r }
yhat.boost <- predict(boost.boston,
    newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

We can perform boosting with a different value of the shrinkage parameter $\lambda$. The default value is $0.001$.

```{r }
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
    distribution = "gaussian", n.trees = 5000,
    interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston,
    newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

# GBM for classification

  We'll use the Iris dataset as a target classification data and prepare it by splitting into the train and test parts.

```{r}
iris2 <-iris[iris$Species!="setosa",]
iris2$Species<-as.numeric(iris2$Species)-2
iris2$Species
indexes = sample(dim(iris2)[1],dim(iris2)[1]*0.8)
train = iris2[indexes, ]
test = iris2[-indexes, ]
```

  We set multinomial distribution, 10 cross-validation fold, and 200 trees.
```{r}

mod_gbm = gbm(Species ~.,
              data = train,
              distribution = "bernoulli",
              cv.folds = 10,
              shrinkage = .01,
              n.minobsinnode = 10,
              n.trees = 200)
 
print(mod_gbm)
summary(mod_gbm)
```
The model is ready, and we'll predict test data.
```{r}
pred = predict.gbm(object = mod_gbm,
                   newdata = test,
                   n.trees = 200,
                   type = "response")
```


Finally, we'll check the confusion matrix.
```{r}
cm = confusionMatrix(factor(test$Species), factor(round(pred)))
print(cm)
```
